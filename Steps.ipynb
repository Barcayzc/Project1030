{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Exploratory Data Analysis (EDA)**: \n",
    "\n",
    "   - Feature engineering (if needed): change or add new x's: x1 -> x1^2\n",
    "     \n",
    "   - Feature selection: eg. 100 features， 选 15 个最有用的features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Overview </center>\n",
    "\n",
    "| *Visualization types* | continuous | categorical |\n",
    "| --- | :-: | :-: |\n",
    "| __continuous__    \t| Hexbin Plot, Contour Plot, 2D Histogram, Pair Plot, Line Plot \t| Strip Plot, Swarm Plot, Point Plot, Bar Plot, Facet Plot|\n",
    "| __categorical__   \t| Strip Plot, Swarm Plot, Point Plot, Bar Plot, Facet Plot |  Grouped Bar Plot, Mosaic Plot, Heatmap, Chi-Square Plot, Alluvial Plot  \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Overview </center>\n",
    "\n",
    "| *Visualization types* |  continuous |  categorical |\n",
    "| --- | :-: | :-: |\n",
    "| __continuous__    \t| scatter plot, heatmap \t| category-specific histograms, box plot, violin plot |\n",
    "| __categorical__   \t| category-specific histograms, box plot, violin plot |  stacked bar plot  \t|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Split the data into different sets**: most often the sets are train, validation, and test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature engineering\n",
    "\n",
    "   - (SMALL features and LARGE points)\n",
    "   - combine features in a simple and automatic way (PolynomialFeatures method in sklearn)\n",
    "   - if n_ftrs << n_points, this can modestly improve the predictive power of your model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]]\n",
      "[[ 1.  0.  1.  0.  0.  1.]\n",
      " [ 1.  2.  3.  4.  6.  9.]\n",
      " [ 1.  4.  5. 16. 20. 25.]]\n",
      "[[ 0.  1.  0.  0.  1.]\n",
      " [ 2.  3.  4.  6.  9.]\n",
      " [ 4.  5. 16. 20. 25.]]\n",
      "[[ 0.  1.  0.]\n",
      " [ 2.  3.  6.]\n",
      " [ 4.  5. 20.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "X = np.arange(6).reshape(3, 2)\n",
    "print(X)\n",
    "\n",
    "poly = PolynomialFeatures(2)\n",
    "print(poly.fit_transform(X)) # [1, a, b, a^2, ab, b^2]\n",
    "poly = PolynomialFeatures(2, include_bias=False) # bias = 1\n",
    "print(poly.fit_transform(X)) # [a, b, a^2, ab, b^2]\n",
    "poly = PolynomialFeatures(2,interaction_only=True, include_bias=False) #interaction: a, b, ab\n",
    "print(poly.fit_transform(X)) # [a, b, ab]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Feature selection\n",
    "    -  (LARGE features and SMALL points)\n",
    "    - you have too many features: n_ftrs > n_points (some algorithms break down)\n",
    "    - if training an ML algorithm is too computationally expensive using all the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The steps\n",
    "\n",
    "**1. Exploratory Data Analysis (EDA)**: you need to understand your data and verify that it doesn't contain errors\n",
    "   - do as much EDA as you can!\n",
    "\n",
    "\n",
    "   - Feature engineering (if needed): change or add new x's: x1 -> x1^2\n",
    "     \n",
    "   - Feature selection: eg. 100 features， 选 15 个最有用的features\n",
    "      \n",
    "\n",
    "**2. Split the data into different sets**: most often the sets are train, validation, and test (or holdout)\n",
    "   - practitioners often make errors in this step!\n",
    "   - you can split the data randomly, based on groups, based on time, or any other non-standard way if necessary to answer your ML question\n",
    "\n",
    "**3. Preprocess the data**: ML models only work if X and Y are numbers! Some ML models additionally require each feature to have 0 mean and 1 standard deviation (standardized features)\n",
    "   - often the original features you get contain strings (for example a gender feature would contain 'male', 'female', 'non-binary', 'unknown') which needs to be transformed into numbers\n",
    "   - often the features are not standardized (e.g., age is between 0 and 100) but it needs to be standardized\n",
    "\n",
    "   - **OneHotEncoder** - converts categorical features into dummy arrays\n",
    "   - **OrdinalEncoder** - converts ordinal features into an integer array\n",
    "   - **MinMaxScaler** - scales continuous variables to be between 0 and 1\n",
    "   - **StandardScaler** - standardizes continuous features by removing the mean and scaling to unit variance\n",
    "\n",
    "   - **Missing values:**\n",
    "     - Missing values in ordinal and categorical features can be automatically dealt by `OneHotEncoder` and `OrdinalEncoder`.  Replace the NaN with a string first\n",
    "     - The missing values in continuous features can be done in one of the following techniques:\n",
    "       - apply multivariate imputation `SimpleImputer`\n",
    "       - apply XGBoost model to a dataset with missing values\n",
    "       - apply the reduced-features model (also called the pattern sub-model approach)\n",
    "\n",
    "\n",
    "    \n",
    "**4. Choose an evaluation metric**: depends on the priorities of the stakeholders\n",
    "   -  Classification evaluation metrics: confusion matrix, accuracy, recall, precision, f-score, ROC, precision-recall curves, log-loss metric\n",
    "      - ROC AUC - ROC Area Under the Curve (算面积 得到一个数值)\n",
    "        - AUC = 1 is a perfect classifier\n",
    "        - AUC > 0.5 is above chance-level predictor\n",
    "        - AUC = 0.5 is a chance-level classifier\n",
    "        - AUC < 0.5 is a bad predictor\n",
    "        - AUC = 0 classifies all points incorrectly\n",
    "   -  Regression evaluation metrics: MSE, RMSE, MAE, R^2\n",
    "\n",
    "   - the model performs very good on the training set but poorly on the validation set when all features are used\n",
    "   - ## Regularization to the rescue!\n",
    "   - let's change the cost function and add a <font color='RED'>penalty term</font> for large ws\n",
    "   - **Lasso regression**: regularize using the l1 norm of w:\n",
    "      \n",
    "$L(w) = \\frac{1}{n}\\sum_{i=1}^{n}[(w_0 + \\sum_{j=1}^{m} w_j  x_{ij}- y_i)^2] + \\color{red}{ \\alpha \\sum_{j=0}^{m}|w_j|}$ \n",
    "      \n",
    "   - **Ridge regression**: regularize using the square of the l2 norm of w:\n",
    "      \n",
    "$L(w) = \\frac{1}{n}\\sum_{i=1}^{n}[(w_0 + \\sum_{j=1}^{m} w_j  x_{ij}- y_i)^2] + \\color{red}{\\alpha \\sum_{j=0}^{m} w_j^2}$\n",
    "\n",
    "   - $\\alpha$ is the regularization parameter (positive number), it describes how much we penalize large ws\n",
    "\n",
    "   - With the cost function changed, the derivatives in gradient descent need to be updated too!   \n",
    "\n",
    "\n",
    "  \n",
    "  <table>\n",
    "    <tr>\n",
    "        <td colspan=\"2\" rowspan=\"2\"></td>\n",
    "        <td colspan=\"2\">Predicted class</td>\t\t\t\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Predicted Negative (0)</td>\n",
    "        <td>Predicted Positive (1)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=\"2\">Actual class</td>\n",
    "        <td>Condition Negative (0)</td>\n",
    "        <td><b>True Negative (TN)</b></td>\n",
    "        <td><b>False Positive (FP)</b></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Condition Positive (1)</td>\n",
    "        <td><b>False Negative (FN)</b></td>\n",
    "        <td><b>True Positive (TP)</b></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "  - accuracy: fraction of data points correctly classified\n",
    "    - a = (TP + TN) / (TP + TN + FP + FN)\n",
    "  - recall: what fraction of the condition positive samples are true positives?\n",
    "    - it measures the ability of the classifier to identify all positive samples\n",
    "    - in binary classification: R = TP / (TP + FN) #想要避免FN FN is expensive. eg: 实际有病但没检测出来\n",
    "  - precision: what fraction of the predicted positive points are true positives?\n",
    "    - it measures the ability of the classifier to not predict a negative sample to be positive\n",
    "    - in binary classification: P = TP / (TP + FP)\n",
    "  - The f_beta score (imbalanced data)\n",
    "  - \n",
    "    Weighted harmonic mean of P and R:\n",
    "    ###  $f_{\\beta} = (1 + \\beta^2) \\frac{P R}{\\beta^2 P + R}$ \n",
    "\n",
    "    If $\\beta = 1$, we have the f1 score:\n",
    "    ###  $f_{1} = 2 \\frac{P R}{P + R}$ \n",
    "\n",
    "    If $\\beta < 1$, more weight to precision. Expensive to act. \n",
    "\n",
    "    If $\\beta > 1$, more weight to recall. Cheap to act. \n",
    "     \n",
    "**5. Choose one or more ML techniques**: it is highly recommended that you try multiple models\n",
    "   - start with simple models like linear or logistic regression\n",
    "   - try also more complex models like nearest neighbors, support vector machines, random forest, etc.\n",
    "\n",
    "   - linear regression: continuous target\n",
    "   - logistic regression: classification target\n",
    "\n",
    "\n",
    "| ML algo | suitable for large datasets? | behavior with respect to outliers | non-linear? | params to tune | smooth predictions | easy to interpret? |\n",
    "| - | :-: | :-: | :-: | :-: | :-: | :-: |\n",
    "| linear regression            \t|              yes             \t|linear extrapolation|      no     \t| l1 and/or l2 reg \t| yes | yes|\n",
    "| logistic regression          \t|              yes             \t|scales with distance from the decision boundary|      no     \t| l1 and/or l2 reg \t| yes | yes|\n",
    "| random forest regression     \t|so so |constant|yes|max_features,  max_depth| no|so so|\n",
    "| random forest classification \t|so so |step-like, difficult to tell|yes|max_features,  max_depth| no|so so|\n",
    "| SVM rbf regression               \t|no|non-linear extrapolation|yes|C, gamma|yes|so so|\n",
    "| SVM rbf classification           \t|no| fifty-fifty | yes| C, gamma| yes| so so |\n",
    "    \n",
    "**6. Tune the hyperparameters of your ML models (aka cross-validation)**\n",
    "   - regularization if over fitting ()\n",
    "   - ML techniques have hyperparameters that you need to optimize to achieve best performance\n",
    "   - for each ML model, decide which parameters to tune and what values to try\n",
    "   - loop through each parameter combination\n",
    "       - train one model for each parameter combination\n",
    "       - evaluate how well the model performs on the validation set\n",
    "   - take the parameter combo that gives the best validation score\n",
    "   - evaluate that model on the test set to report how well the model is expected to perform on previously unseen data\n",
    "    \n",
    "**7. Interpret your model**: black boxes are often not useful\n",
    "   - check if your model uses features that make sense (excellent tool for debugging)\n",
    "   - often model predictions are not enough, you need to be able to explain how the model arrived to a particular prediction (e.g., in health care)\n",
    "   - Global feature importance metrics: \n",
    "     - Permutation feature importance \n",
    "     - coefficients of linear models\n",
    "   - Local feature importance metrics: \n",
    "     - `Shap` values\n",
    "     - Locally Interpretable Model-agnostic Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC \n",
    "LogisticRegression \n",
    "XGBoost \n",
    "RandomForestClassifier \n",
    "\n",
    "For imbalanced classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#np.random.seed(0)\n",
    "\n",
    "df = pd.read_csv('data/adult_data.csv')\n",
    "\n",
    "# let's separate the feature matrix X, and target variable y\n",
    "y = df['gross-income'] # remember, we want to predict who earns more than 50k or less than 50k\n",
    "X = df.loc[:, df.columns != 'gross-income'] # all other columns are features\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "# first split to separate out the training set\n",
    "X_train, X_other, y_train, y_other = train_test_split(X,y,train_size = 0.6,random_state=random_state)\n",
    "\n",
    "# second split to separate out the validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_other,y_other,train_size = 0.5,random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect which encoder to use on each feature\n",
    "# target 永远不preprocess\n",
    "# needs to be done manually\n",
    "ordinal_ftrs = ['education'] \n",
    "ordinal_cats = [[' Preschool',' 1st-4th',' 5th-6th',' 7th-8th',' 9th',' 10th',' 11th',' 12th',' HS-grad',\\\n",
    "                ' Some-college',' Assoc-voc',' Assoc-acdm',' Bachelors',' Masters',' Prof-school',' Doctorate']]\n",
    "onehot_ftrs = ['workclass','marital-status','occupation','relationship','race','sex','native-country']\n",
    "minmax_ftrs = ['age','hours-per-week']\n",
    "std_ftrs = ['capital-gain','capital-loss']\n",
    "\n",
    "# collect all the encoders\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('ord', OrdinalEncoder(categories = ordinal_cats), ordinal_ftrs),\n",
    "        ('onehot', OneHotEncoder(sparse_output=False,handle_unknown='ignore'), onehot_ftrs),\n",
    "        ('minmax', MinMaxScaler(), minmax_ftrs),\n",
    "        ('std', StandardScaler(), std_ftrs)])\n",
    "\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor)]) # for now we only preprocess \n",
    "                                                       # later on we will add other steps here\n",
    "\n",
    "X_train_prep = clf.fit_transform(X_train)\n",
    "X_val_prep = clf.transform(X_val)\n",
    "X_test_prep = clf.transform(X_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train_prep.shape)\n",
    "print(X_train_prep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. baseline_accuracy\n",
    "2. baseline in SHAP\n",
    "3. train cv test?\n",
    "4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.metrics import make_scorer, fbeta_score\n",
    "# import joblib\n",
    "\n",
    "\n",
    "# def MLpipe_KFold_FBeta(X, y, preprocessor, ML_algo, param_grid):\n",
    "#     \"\"\"\n",
    "#     Function to perform KFold cross-validation, hyperparameter tuning with GridSearchCV, and evaluate with F-beta score.\n",
    "#     Args:\n",
    "#         X (pd.DataFrame): Feature matrix.\n",
    "#         y (pd.Series): Target variable.\n",
    "#         preprocessor (ColumnTransformer): Preprocessor pipeline for data transformation.\n",
    "#         ML_algo (Estimator): Uninitialized machine learning algorithm (e.g., RandomForestClassifier()).\n",
    "#         param_grid (dict): Parameter grid for hyperparameter tuning.\n",
    "#     Returns:\n",
    "#         test_scores (list): List of 10 test fbeta scores (1 per random state).\n",
    "#         best_models (list): List of 10 best models (1 per random state).\n",
    "#     \"\"\"\n",
    "#     # Lists to store test scores and best models for each random state\n",
    "#     test_scores = []\n",
    "#     best_models = []\n",
    "\n",
    "#     # Define the fbeta scoring function\n",
    "#     fbeta_scorer = make_scorer(fbeta_score, beta=1, greater_is_better=True)\n",
    "\n",
    "#     # Loop through 10 different random states\n",
    "#     for random_state in range(10):\n",
    "#         print(f\"\\n========== Random State: {random_state} ==========\")\n",
    "\n",
    "#         # Split the data into 'other' (train + validation) and test (80/20 split)\n",
    "#         X_other, X_test, y_other, y_test = train_test_split(\n",
    "#             X, y, test_size=0.2, stratify=y, random_state=random_state\n",
    "#         )\n",
    "#         # print(f\"Data split - X_other: {X_other.shape}, X_test: {X_test.shape}\")\n",
    "\n",
    "#         # Apply KFold with 4 splits on the 'other' set\n",
    "#         kf = KFold(n_splits=4, shuffle=True, random_state=random_state)\n",
    "\n",
    "#         # Create a pipeline with the preprocessor and ML algorithm\n",
    "#         pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "#                                     ('model', ML_algo)])\n",
    "        \n",
    "#         # Use GridSearchCV for hyperparameter tuning\n",
    "#         grid_search = GridSearchCV(\n",
    "#             estimator=pipeline,\n",
    "#             param_grid=param_grid,\n",
    "#             cv=kf,\n",
    "#             scoring=fbeta_scorer,\n",
    "#             n_jobs=-1\n",
    "#         )\n",
    "\n",
    "#         # Fit the pipeline with GridSearchCV\n",
    "#         print(\"Performing GridSearchCV...\")\n",
    "#         grid_search.fit(X_other, y_other)\n",
    "\n",
    "#         # Print the best parameters found\n",
    "#         print(f\"Best parameters found for Random State {random_state}: {grid_search.best_params_}\")\n",
    "\n",
    "#         # Save the best model from this random state\n",
    "#         best_model = grid_search.best_estimator_\n",
    "#         best_models.append(best_model)\n",
    "        \n",
    "#         # Save the model using joblib\n",
    "#         model_filename = f\"best_model_state_{random_state}.pkl\"\n",
    "#         joblib.dump(best_model, model_filename)\n",
    "#         print(f\"Saved best model for Random State {random_state} as {model_filename}\")\n",
    "\n",
    "#         # Predict on the test set and calculate fbeta score\n",
    "#         y_test_pred = best_model.predict(X_test)\n",
    "#         test_fbeta = fbeta_score(y_test, y_test_pred, beta=1)\n",
    "#         test_scores.append(test_fbeta)\n",
    "#         print(f\"Test F-beta score for Random State {random_state}: {test_fbeta:.4f}\")\n",
    "\n",
    "#     # Calculate mean and standard deviation of test scores across all random states\n",
    "#     mean_test_score = np.mean(test_scores)\n",
    "#     std_test_score = np.std(test_scores)\n",
    "#     print(\"\\n========== Summary of Test Scores ==========\")\n",
    "#     print(f\"Mean Test F-beta Score: {mean_test_score:.4f}\")\n",
    "#     print(f\"Standard Deviation of Test Scores: {std_test_score:.4f}\")\n",
    "\n",
    "#     # Return the list of test scores and best models\n",
    "#     return test_scores, best_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, average_precision_score\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "def MLpipe_KFold_AUC_PR(X, y, preprocessor, ML_algo, param_grid):\n",
    "    \"\"\"\n",
    "    Function to perform KFold cross-validation, hyperparameter tuning with GridSearchCV, and evaluate with AUC-PR.\n",
    "    Args:\n",
    "        X (pd.DataFrame): Feature matrix.\n",
    "        y (pd.Series): Target variable.\n",
    "        preprocessor (ColumnTransformer): Preprocessor pipeline for data transformation.\n",
    "        ML_algo (Estimator): Uninitialized machine learning algorithm (e.g., RandomForestClassifier()).\n",
    "        param_grid (dict): Parameter grid for hyperparameter tuning.\n",
    "    Returns:\n",
    "        test_scores (list): List of 10 test AUC-PR scores (1 per random state).\n",
    "        best_models (list): List of 10 best models (1 per random state).\n",
    "    \"\"\"\n",
    "    # Lists to store test scores and best models for each random state\n",
    "    test_scores = []\n",
    "    best_models = []\n",
    "\n",
    "    # Define the AUC-PR scoring function\n",
    "    auc_pr_scorer = make_scorer(average_precision_score, needs_proba=True)\n",
    "\n",
    "    random_states = []\n",
    "    # Loop through 5 different random states\n",
    "\n",
    "    for i in range(5):\n",
    "        random_states.append(42 * i)\n",
    "\n",
    "\n",
    "    for random_state in random_states:\n",
    "        print(f\"\\n========== Random State: {random_state} ==========\")\n",
    "\n",
    "        # Split the data into 'other' (train + validation) and test (80/20 split)\n",
    "        X_other, X_test, y_other, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, stratify=y, random_state=random_state\n",
    "        )\n",
    "        # print(f\"Data split - X_other: {X_other.shape}, X_test: {X_test.shape}\")\n",
    "\n",
    "        # Apply KFold with 4 splits on the 'other' set\n",
    "        kf = KFold(n_splits=4, shuffle=True, random_state=random_state)\n",
    "\n",
    "        # Create a pipeline with the preprocessor and ML algorithm\n",
    "        pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                    ('model', ML_algo)])\n",
    "        \n",
    "        # Use GridSearchCV for hyperparameter tuning\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_grid=param_grid,\n",
    "            cv=kf,\n",
    "            scoring=auc_pr_scorer,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        # Fit the pipeline with GridSearchCV\n",
    "        print(\"Performing GridSearchCV...\")\n",
    "        grid_search.fit(X_other, y_other)\n",
    "\n",
    "        # Print the best parameters found\n",
    "        print(f\"Best parameters found for Random State {random_state}: {grid_search.best_params_}\")\n",
    "\n",
    "        # Save the best model from this random state\n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_models.append(best_model)\n",
    "        \n",
    "        # Save the model using joblib\n",
    "        model_filename = f\"best_model_state_{random_state}.pkl\"\n",
    "        joblib.dump(best_model, model_filename)\n",
    "        print(f\"Saved best model for Random State {random_state} as {model_filename}\")\n",
    "\n",
    "        # Predict probabilities on the test set and calculate AUC-PR\n",
    "        y_test_prob = best_model.predict_proba(X_test)[:, 1]  # Use probabilities for the positive class\n",
    "        test_auc_pr = average_precision_score(y_test, y_test_prob)\n",
    "        test_scores.append(test_auc_pr)\n",
    "        print(f\"Test AUC-PR score for Random State {random_state}: {test_auc_pr:.4f}\")\n",
    "\n",
    "    # Calculate mean and standard deviation of test scores across all random states\n",
    "    mean_test_score = np.mean(test_scores)\n",
    "    std_test_score = np.std(test_scores)\n",
    "    print(\"\\n========== Summary of Test Scores ==========\")\n",
    "    print(f\"Mean Test AUC-PR Score: {mean_test_score:.4f}\")\n",
    "    print(f\"Standard Deviation of Test AUC-PR Score: {std_test_score:.4f}\")\n",
    "\n",
    "    # Return the list of test scores and best models\n",
    "    return test_scores, best_models\n",
    "    \n",
    "\n",
    "results = {}\n",
    "\n",
    "# Loop through each model, run the pipeline, and collect results\n",
    "for model_name, (model, param_grid) in models.items():\n",
    "    print(f\"\\n--- Running model: {model_name} ---\")\n",
    "    test_scores, best_models = MLpipe_KFold_AUC_PR(X, y, preprocessor, model, param_grid)\n",
    "    mean_auc_pr = np.mean(test_scores)\n",
    "    std_auc_pr = np.std(test_scores)\n",
    "    results[model_name] = (mean_auc_pr, std_auc_pr)  # Store results in dictionary\n",
    "    print(f\"\\nMean Test AUC-PR Score for {model_name}: {mean_auc_pr:.4f}\")\n",
    "    print(f\"Standard Deviation of Test AUC-PR Score for {model_name}: {std_auc_pr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'XGBClassifier': (\n",
    "        XGBClassifier(eval_metric='logloss'),\n",
    "        {\"model__learning_rate\": [0.03],\n",
    "        \"model__n_estimators\": [10000],\n",
    "        \"model__seed\": [0],\n",
    "        # \"model__reg_alpha\": [0e0, 1e-2, 1e-1, 1e0, 1e1, 1e2],\n",
    "        # \"model__reg_lambda\": [0e0, 1e-2, 1e-1, 1e0, 1e1, 1e2],\n",
    "        # \"model__max_depth\": [1,3,10,30,100],\n",
    "        \"model__reg_alpha\": [1e-1, 1e0, 1e1],\n",
    "        \"model__reg_lambda\": [ 1e-1, 1e0, 1e1,],\n",
    "        \"model__max_depth\": [1,3,10],\n",
    "        \"model__colsample_bytree\": [0.9],              \n",
    "        \"model__subsample\": [0.66]}\n",
    "    )\n",
    "\n",
    "'SVC': (\n",
    "    SVC(probability=True),\n",
    "    {'model__gamma': [1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3],\n",
    "    'model__C': [1e-2, 1e-1, 1e0, 1e1, 1e2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, average_precision_score\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "def MLpipe_KFold_AUC_PR(X, y, preprocessor, ML_algo, param_grid):\n",
    "    \"\"\"\n",
    "    Function to perform KFold cross-validation, hyperparameter tuning with GridSearchCV, and evaluate with AUC-PR.\n",
    "    Args:\n",
    "        X (pd.DataFrame): Feature matrix.\n",
    "        y (pd.Series): Target variable.\n",
    "        preprocessor (ColumnTransformer): Preprocessor pipeline for data transformation.\n",
    "        ML_algo (Estimator): Uninitialized machine learning algorithm (e.g., RandomForestClassifier()).\n",
    "        param_grid (dict): Parameter grid for hyperparameter tuning.\n",
    "    Returns:\n",
    "        test_scores (list): List of 10 test AUC-PR scores (1 per random state).\n",
    "        best_models (list): List of 10 best models (1 per random state).\n",
    "    \"\"\"\n",
    "    # Lists to store test scores and best models for each random state\n",
    "    test_scores = []\n",
    "    best_models = []\n",
    "\n",
    "    # Define the AUC-PR scoring function\n",
    "    auc_pr_scorer = make_scorer(average_precision_score, needs_proba=True)\n",
    "\n",
    "    random_states = []\n",
    "    # Loop through 5 different random states\n",
    "\n",
    "    for i in range(5):\n",
    "        random_states.append(42 * i)\n",
    "\n",
    "\n",
    "    for random_state in random_states:\n",
    "        print(f\"\\n========== Random State: {random_state} ==========\")\n",
    "\n",
    "        # Split the data into 'other' (train + validation) and test (80/20 split)\n",
    "        X_other, X_test, y_other, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, stratify=y, random_state=random_state\n",
    "        )\n",
    "        # print(f\"Data split - X_other: {X_other.shape}, X_test: {X_test.shape}\")\n",
    "\n",
    "        # Apply KFold with 4 splits on the 'other' set\n",
    "        kf = KFold(n_splits=4, shuffle=True, random_state=random_state)\n",
    "\n",
    "        # Create a pipeline with the preprocessor and ML algorithm\n",
    "        pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                    ('model', ML_algo)])\n",
    "        \n",
    "        # Use GridSearchCV for hyperparameter tuning\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_grid=param_grid,\n",
    "            cv=kf,\n",
    "            scoring=auc_pr_scorer,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        # Fit the pipeline with GridSearchCV\n",
    "        print(\"Performing GridSearchCV...\")\n",
    "        grid_search.fit(X_other, y_other)\n",
    "\n",
    "        # Print the best parameters found\n",
    "        print(f\"Best parameters found for Random State {random_state}: {grid_search.best_params_}\")\n",
    "\n",
    "        # Save the best model from this random state\n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_models.append(best_model)\n",
    "        \n",
    "        # Save the model using joblib\n",
    "        model_filename = f\"best_model_state_{random_state}.pkl\"\n",
    "        joblib.dump(best_model, model_filename)\n",
    "        print(f\"Saved best model for Random State {random_state} as {model_filename}\")\n",
    "\n",
    "        # Predict probabilities on the test set and calculate AUC-PR\n",
    "        y_test_prob = best_model.predict_proba(X_test)[:, 1]  # Use probabilities for the positive class\n",
    "        test_auc_pr = average_precision_score(y_test, y_test_prob)\n",
    "        test_scores.append(test_auc_pr)\n",
    "        print(f\"Test AUC-PR score for Random State {random_state}: {test_auc_pr:.4f}\")\n",
    "\n",
    "    # Calculate mean and standard deviation of test scores across all random states\n",
    "    mean_test_score = np.mean(test_scores)\n",
    "    std_test_score = np.std(test_scores)\n",
    "    print(\"\\n========== Summary of Test Scores ==========\")\n",
    "    print(f\"Mean Test AUC-PR Score: {mean_test_score:.4f}\")\n",
    "    print(f\"Standard Deviation of Test AUC-PR Score: {std_test_score:.4f}\")\n",
    "\n",
    "    # Return the list of test scores and best models\n",
    "    return test_scores, best_models, X_test, y_test\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the preprocessor\n",
    "ordinal_ftrs = ['Episode_Number']\n",
    "minmax_ftrs = ['Age']\n",
    "passthrough_ftrs = ['Sex']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('ord', OrdinalEncoder(), ordinal_ftrs),\n",
    "        ('minmax', MinMaxScaler(), minmax_ftrs)],\n",
    "    remainder='passthrough'  # Pass through the remaining columns untransformed\n",
    ")\n",
    "\n",
    "# class_weight = 'balanced'\n",
    "\n",
    "# Define models and parameter grids\n",
    "models = {\n",
    "    'LogisticRegression': (\n",
    "        LogisticRegression(solver='saga'),  \n",
    "        {'model__penalty': ['elasticnet'],\n",
    "         'model__C': [1e-2, 1e-1, 1e0, 1e1],\n",
    "         'model__l1_ratio': [0, 0.25, 0.5, 0.75, 1]}),\n",
    "         \n",
    "    'RandomForestClassifier':(\n",
    "        RandomForestClassifier(),                         \n",
    "        {'model__max_depth': [4, 5, 6],\n",
    "        'model__max_features': [0.65, 0.7, 0.75, 0.8, 0.85]}),\n",
    "\n",
    "    'KNeighborsClassifier':(\n",
    "    KNeighborsClassifier(),\n",
    "    {'model__n_neighbors': [1000, 1500, 1700],\n",
    "    'model__metric': ['euclidean', 'manhattan'],\n",
    "    'model__weights': ['uniform']}),\n",
    "\n",
    "    'XGBClassifier': (\n",
    "        XGBClassifier(eval_metric='logloss'),\n",
    "        {\"model__reg_alpha\": [1e-1, 1e0, 1e1],\n",
    "        \"model__reg_lambda\": [ 1e-2, 1e-1, 1e0, 1e1],\n",
    "        \"model__max_depth\": [3,5,10,15]}),\n",
    "\n",
    "    # 'SVC': (\n",
    "    # SVC(probability=True),\n",
    "    # {'model__gamma': [1e-1, 1e0, 1e1],\n",
    "    # 'model__C': [1e-1, 1e0, 1e1]})\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Loop through each model, run the pipeline, and collect results\n",
    "for model_name, (model, param_grid) in models.items():\n",
    "    print(f\"\\n--- Running model: {model_name} ---\")\n",
    "    test_scores, best_models = MLpipe_KFold_AUC_PR(X, y, preprocessor, model, param_grid)\n",
    "    mean_auc_pr = np.mean(test_scores)\n",
    "    std_auc_pr = np.std(test_scores)\n",
    "    results[model_name] = (mean_auc_pr, std_auc_pr)  # Store results in dictionary\n",
    "    print(f\"\\nMean Test AUC-PR Score for {model_name}: {mean_auc_pr:.4f}\")\n",
    "    print(f\"Standard Deviation of Test AUC-PR Score for {model_name}: {std_auc_pr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import make_scorer, average_precision_score\n",
    "# from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# import numpy as np\n",
    "# import joblib\n",
    "\n",
    "# def MLpipe_KFold_AUC_PR(X, y, preprocessor, ML_algo, param_grid):\n",
    "#     \"\"\"\n",
    "#     Function to perform KFold cross-validation, hyperparameter tuning with GridSearchCV, and evaluate with AUC-PR.\n",
    "#     Args:\n",
    "#         X (pd.DataFrame): Feature matrix.\n",
    "#         y (pd.Series): Target variable.\n",
    "#         preprocessor (ColumnTransformer): Preprocessor pipeline for data transformation.\n",
    "#         ML_algo (Estimator): Uninitialized machine learning algorithm (e.g., RandomForestClassifier()).\n",
    "#         param_grid (dict): Parameter grid for hyperparameter tuning.\n",
    "#     Returns:\n",
    "#         best_test_score (float): The highest test AUC-PR score across all random states.\n",
    "#         best_model (Estimator): The best-performing model with the highest test AUC-PR score.\n",
    "#         X_test (pd.DataFrame): Test features for later use.\n",
    "#         y_test (pd.Series): Test targets for later use.\n",
    "#     \"\"\"\n",
    "\n",
    "\n",
    "#     # Initialize variables to track the best model\n",
    "#     best_test_score = -np.inf  # Negative infinity to ensure any valid score will be higher\n",
    "#     best_model = None\n",
    "#     best_random_state = None\n",
    "#     test_scores = []\n",
    "\n",
    "#     # Define the AUC-PR scoring function\n",
    "#     auc_pr_scorer = make_scorer(average_precision_score, needs_proba=True)\n",
    "\n",
    "#     random_states = [42 * i for i in range(5)]  # Generate random states\n",
    "\n",
    "#     for random_state in random_states:\n",
    "#         print(f\"\\n========== Random State: {random_state} ==========\")\n",
    "\n",
    "#         # Split the data into 'other' (train + validation) and test (80/20 split)\n",
    "#         X_other, X_test, y_other, y_test = train_test_split(\n",
    "#             X, y, test_size=0.2, stratify=y, random_state=random_state\n",
    "#         )\n",
    "\n",
    "#         # Apply KFold with 4 splits on the 'other' set\n",
    "#         kf = KFold(n_splits=4, shuffle=True, random_state=random_state)\n",
    "\n",
    "#         # Create a pipeline with the preprocessor and ML algorithm\n",
    "#         pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "#                                     ('model', ML_algo)])\n",
    "        \n",
    "#         # Use GridSearchCV for hyperparameter tuning\n",
    "#         grid_search = GridSearchCV(\n",
    "#             estimator=pipeline,\n",
    "#             param_grid=param_grid,\n",
    "#             cv=kf,\n",
    "#             scoring=auc_pr_scorer,\n",
    "#             n_jobs=-1\n",
    "#         )\n",
    "\n",
    "#         # Fit the pipeline with GridSearchCV\n",
    "#         print(\"Performing GridSearchCV...\")\n",
    "#         grid_search.fit(X_other, y_other)\n",
    "\n",
    "#         # Print the best parameters found\n",
    "#         print(f\"Best parameters found for Random State {random_state}: {grid_search.best_params_}\")\n",
    "\n",
    "#         # Get the best model from this random state\n",
    "#         best_model_for_state = grid_search.best_estimator_\n",
    "\n",
    "#         # Predict probabilities on the test set and calculate AUC-PR\n",
    "#         y_test_prob = best_model_for_state.predict_proba(X_test)[:, 1]  # Use probabilities for the positive class\n",
    "#         test_auc_pr = average_precision_score(y_test, y_test_prob)\n",
    "#         test_scores.append(test_auc_pr)\n",
    "#         print(f\"Test AUC-PR score for Random State {random_state}: {test_auc_pr:.4f}\")\n",
    "\n",
    "#         # Update the best model if the current model performs better\n",
    "#         if test_auc_pr > best_test_score:\n",
    "#             best_test_score = test_auc_pr\n",
    "#             best_model = best_model_for_state\n",
    "#             best_random_state = random_state\n",
    "\n",
    "\n",
    "#     # Calculate mean and standard deviation of test scores across all random states\n",
    "#     mean_test_score = np.mean(test_scores)\n",
    "#     std_test_score = np.std(test_scores)\n",
    "#     print(\"\\n========== Summary of Test Scores ==========\")\n",
    "#     print(f\"Mean Test AUC-PR Score: {mean_test_score:.4f}\")\n",
    "#     print(f\"Standard Deviation of Test AUC-PR Score: {std_test_score:.4f}\")\n",
    "\n",
    "\n",
    "#     print(f\"\\n========== Best Model Summary ==========\")\n",
    "#     print(f\"Best Random State: {best_random_state}\")\n",
    "#     print(f\"Best Test AUC-PR Score: {best_test_score:.4f}\")\n",
    "\n",
    "\n",
    "#     # # Save the best model\n",
    "#     # model_filename = f\"best_model_overall.pkl\"\n",
    "#     # joblib.dump(best_model, model_filename)\n",
    "#     # print(f\"Saved best overall model as {model_filename}\")\n",
    "\n",
    "#     # Return the best test score, best model, and test data\n",
    "#     return best_test_score, test_scores, best_model, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, average_precision_score, precision_recall_curve, auc, fbeta_score\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "def MLpipe_KFold_AUC_PR(X, y, preprocessor, ML_algo, param_grid):\n",
    "    \"\"\"\n",
    "    Function to perform KFold cross-validation, hyperparameter tuning with GridSearchCV, and evaluate with AUC-PR.\n",
    "    Args:\n",
    "        X (pd.DataFrame): Feature matrix.\n",
    "        y (pd.Series): Target variable.\n",
    "        preprocessor (ColumnTransformer): Preprocessor pipeline for data transformation.\n",
    "        ML_algo (Estimator): Uninitialized machine learning algorithm (e.g., RandomForestClassifier()).\n",
    "        param_grid (dict): Parameter grid for hyperparameter tuning.\n",
    "    Returns:\n",
    "        best_test_score (float): The highest test AUC-PR score across all random states.\n",
    "        best_model (Estimator): The best-performing model with the highest test AUC-PR score.\n",
    "        X_test (pd.DataFrame): Test features for later use.\n",
    "        y_test (pd.Series): Test targets for later use.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Initialize variables to track the best model\n",
    "    best_test_score = -np.inf  # Negative infinity to ensure any valid score will be higher\n",
    "    best_model = None\n",
    "    best_random_state = None\n",
    "    test_scores = []\n",
    "\n",
    "    # Define the AUC-PR scoring function\n",
    "    # auc_pr_scorer = make_scorer(average_precision_score, needs_proba=True)\n",
    "\n",
    "        # Define the scoring function for GridSearchCV\n",
    "    def auc_pr_scorer(estimator, X_val, y_val):\n",
    "        y_val_prob = estimator.predict_proba(X_val)[:, 1]  # Probabilities for the positive class\n",
    "        precision, recall, _ = precision_recall_curve(y_val, y_val_prob)\n",
    "        return auc(recall, precision)  # Calculate AUC-PR\n",
    "    \n",
    "\n",
    "    random_states = [42 * i for i in range(5)]  # Generate random states\n",
    "\n",
    "    for random_state in random_states:\n",
    "        print(f\"\\n========== Random State: {random_state} ==========\")\n",
    "\n",
    "        # Split the data into 'other' (train + validation) and test (80/20 split)\n",
    "        X_other, X_test, y_other, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, stratify=y, random_state=random_state\n",
    "        )\n",
    "\n",
    "        # Apply KFold with 4 splits on the 'other' set\n",
    "        kf = KFold(n_splits=4, shuffle=True, random_state=random_state)\n",
    "\n",
    "        # Create a pipeline with the preprocessor and ML algorithm\n",
    "        pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                   ('scaler', StandardScaler()),\n",
    "                                    ('model', ML_algo)\n",
    "                                    ])\n",
    "        \n",
    "        # Use GridSearchCV for hyperparameter tuning\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_grid=param_grid,\n",
    "            cv=kf,\n",
    "            scoring=auc_pr_scorer,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        # Fit the pipeline with GridSearchCV\n",
    "        print(\"Performing GridSearchCV...\")\n",
    "        grid_search.fit(X_other, y_other)\n",
    "\n",
    "        # Print the best parameters found\n",
    "        print(f\"Best parameters found for Random State {random_state}: {grid_search.best_params_}\")\n",
    "\n",
    "        # Get the best model from this random state\n",
    "        best_model_for_state = grid_search.best_estimator_\n",
    "        \n",
    "        # Predict probabilities on the test set and calculate AUC-PR\n",
    "        y_test_prob = best_model_for_state.predict_proba(X_test)[:, 1]  # Use probabilities for the positive class\n",
    "        # test_auc_pr = fbeta_score(y_test, y_test_prob, beta=1)\n",
    "        # test_auc_pr = average_precision_score(y_test, y_test_prob)\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_test_prob)\n",
    "        test_auc_pr = auc(recall, precision)  # Calculate AUC-PR\n",
    "        test_scores.append(test_auc_pr)\n",
    "        print(f\"Test AUC-PR score for Random State {random_state}: {test_auc_pr:.4f}\")\n",
    "\n",
    "        # Update the best model if the current model performs better\n",
    "        if test_auc_pr > best_test_score:\n",
    "            best_test_score = test_auc_pr\n",
    "            best_model = best_model_for_state\n",
    "            best_random_state = random_state\n",
    "\n",
    "\n",
    "    # Calculate mean and standard deviation of test scores across all random states\n",
    "    mean_test_score = np.mean(test_scores)\n",
    "    std_test_score = np.std(test_scores)\n",
    "    print(\"\\n========== Summary of Test Scores ==========\")\n",
    "    print(f\"Mean Test AUC-PR Score: {mean_test_score:.4f}\")\n",
    "    print(f\"Standard Deviation of Test AUC-PR Score: {std_test_score:.4f}\")\n",
    "\n",
    "\n",
    "    print(f\"\\n========== Best Model Summary ==========\")\n",
    "    print(f\"Best Random State: {best_random_state}\")\n",
    "    print(f\"Best Test AUC-PR Score: {best_test_score:.4f}\")\n",
    "\n",
    "\n",
    "    # # Save the best model\n",
    "    # model_filename = f\"best_model_overall.pkl\"\n",
    "    # joblib.dump(best_model, model_filename)\n",
    "    # print(f\"Saved best overall model as {model_filename}\")\n",
    "\n",
    "    # Return the best test score, best model, and test data\n",
    "    return best_test_score, test_scores, best_model, X_test, y_test\n",
    "\n",
    "\n",
    "for model_name, (model, param_grid) in models.items():\n",
    "    print(f\"\\n--- Running model: {model_name} ---\")\n",
    "    best_test_score, test_scores, best_model, X_test, y_test = MLpipe_KFold_AUC_PR(X, y, preprocessor, model, param_grid)\n",
    "\n",
    "    # Store the results for the current model\n",
    "    mean_auc_pr = np.mean(test_scores)\n",
    "    std_auc_pr = np.std(test_scores)\n",
    "    results[model_name] = (mean_auc_pr, std_auc_pr)\n",
    "\n",
    "    # Update the overall best model\n",
    "    if best_test_score > highest_mean_score:\n",
    "        highest_mean_score = best_test_score\n",
    "        best_overall_model = best_model\n",
    "        best_X_test, best_y_test = X_test, y_test  # Save test data for best overall model\n",
    "        best_random_state = 42 * np.argmax(test_scores)  # Capture the random state\n",
    "\n",
    "    # Print results for the current model\n",
    "    print(f\"\\nMean Test AUC-PR Score for {model_name}: {mean_auc_pr:.4f}\")\n",
    "    print(f\"Standard Deviation of Test AUC-PR Score for {model_name}: {std_auc_pr:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Print sorted results\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1][0], reverse=True)\n",
    "print(\"\\n========== Final Results Sorted by Mean Test AUC-PR Score ==========\")\n",
    "for model_name, (mean_auc_pr, std_auc_pr) in sorted_results:\n",
    "    print(f\"{model_name}: Mean AUC-PR = {mean_auc_pr:.4f}, Std Dev = {std_auc_pr:.4f}\")\n",
    "\n",
    "# Print the best overall model with random state\n",
    "print(\"\\n========== Best Overall Model ==========\")\n",
    "print(f\"Model: {best_overall_model}\")\n",
    "print(f\"Highest Test AUC-PR Score: {highest_mean_score:.4f}\")\n",
    "print(f\"Corresponding Random State: {best_random_state}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data1030",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
